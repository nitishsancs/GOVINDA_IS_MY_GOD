# GOVINDA V2
### File name Govinda coz only god can help this code ðŸ˜­ðŸ˜­ðŸ˜­ -Nitish, 2026

A **vectorless**, **structure-first** RAG system for analyzing complex regulatory and financial PDF documents using a hierarchical tree structure, multi-hop reasoning, and LLM-powered retrieval.

## Philosophy

GOVINDA V2 is built on the principle that **document structure is more valuable than semantic embeddings** for regulatory documents. Instead of vector similarity, it uses:

- **Hierarchical tree representation** (Chapters â†’ Sections â†’ Clauses â†’ Sub-clauses)
- **LLM-guided tree reasoning** (Locate â†’ Read â†’ Reflect â†’ Synthesize)
- **Cross-reference following** to trace linked provisions
- **Multi-hop planning** for complex questions requiring multiple documents or sections

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         FRONTEND (Next.js 16)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Document     â”‚  â”‚ Chat         â”‚  â”‚ History      â”‚  â”‚ Research     â”‚ â”‚
â”‚  â”‚ Library      â”‚  â”‚ Interface    â”‚  â”‚ Management   â”‚  â”‚ Chat         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚ HTTP / REST API
                                â”‚ with ngrok-skip-browser-warning header
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BACKEND (FastAPI + Uvicorn)                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Document     â”‚  â”‚ Query        â”‚  â”‚ Conversation â”‚  â”‚ Actionable   â”‚   â”‚
â”‚  â”‚ Management   â”‚  â”‚ Processing   â”‚  â”‚ Store        â”‚  â”‚ Extraction   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         RAG PIPELINE (Multi-Agent System)                 â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                        QA Engine (Orchestrator)                     â”‚  â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚  â”‚
â”‚  â”‚    â”‚  Phase 1    â”‚    â”‚  Phase 2    â”‚    â”‚  Phase 3    â”‚            â”‚  â”‚
â”‚  â”‚    â”‚  Retrieval  â”‚â”€â”€â”€â–¶  Synthesis  â”€â”€â”€â–¶   Verification             â”‚  â”‚
â”‚  â”‚    â”‚  (~16s)     â”‚    â”‚  (~100-180s)â”‚    â”‚             â”‚            â”‚  â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Router       â”‚  â”‚ Reflector    â”‚  â”‚ Synthesizer  â”‚  â”‚ Verifier     â”‚   â”‚
â”‚  â”‚ (Locate/Read)â”‚  â”‚ (Gap Fill)   â”‚  â”‚ (Generate)   â”‚  â”‚ (Validate)   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ Planner      â”‚  â”‚ Cross-Ref    â”‚  â”‚ Definition   â”‚                     â”‚
â”‚  â”‚ (Multi-Hop)  â”‚  â”‚ Follower     â”‚  â”‚ Injector     â”‚                     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA LAYER (MongoDB Atlas)                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ Document     â”‚  â”‚ Query        â”‚  â”‚ Conversation â”‚  â”‚ Actionable   â”‚   â”‚
â”‚  â”‚ Trees        â”‚  â”‚ Records      â”‚  â”‚ History      â”‚  â”‚ Items        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ GridFS (PDF binary storage)                                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Project Structure

```
govinda_v2/
â”œâ”€â”€ app_backend/           # FastAPI backend entry point
â”‚   â””â”€â”€ main.py           # Main FastAPI app with all endpoints
â”œâ”€â”€ agents/               # LLM agent implementations
â”‚   â”œâ”€â”€ qa_engine.py     # Main QA orchestrator (6-phase pipeline)
â”‚   â”œâ”€â”€ corpus_qa_engine.py  # Cross-document QA engine
â”‚   â”œâ”€â”€ synthesizer.py    # Answer synthesis with citations
â”‚   â”œâ”€â”€ verifier.py       # Answer verification and validation
â”‚   â”œâ”€â”€ planner.py        # Multi-hop query planning
â”‚   â”œâ”€â”€ actionable_extractor.py  # Compliance actionable extraction
â”‚   â””â”€â”€ qa_engine.py     # Document-specific QA engine
â”œâ”€â”€ retrieval/            # RAG retrieval components
â”‚   â”œâ”€â”€ router.py        # StructuralRouter (Locate + Read phases)
â”‚   â”œâ”€â”€ corpus_router.py  # Cross-document routing
â”‚   â”œâ”€â”€ retrieval_reflector.py  # Evidence gap detection and fill
â”‚   â”œâ”€â”€ query_classifier.py     # Query type classification
â”‚   â”œâ”€â”€ query_expander.py       # Query decomposition
â”‚   â”œâ”€â”€ locator.py       # Tree-based node location
â”‚   â”œâ”€â”€ reader.py        # Content reading and expansion
â”‚   â”œâ”€â”€ cross_ref_follower.py   # Cross-reference resolution
â”‚   â””â”€â”€ definition_injector.py  # Definition term injection
â”œâ”€â”€ ingestion/            # PDF processing pipeline
â”‚   â”œâ”€â”€ pipeline.py      # Main ingestion orchestrator
â”‚   â”œâ”€â”€ parser.py        # PDF text and table extraction (PyMuPDF)
â”‚   â”œâ”€â”€ tree_builder.py  # Document tree construction
â”‚   â””â”€â”€ chunker.py       # Node token management and splitting
â”œâ”€â”€ models/               # Pydantic/dataclass data models
â”‚   â”œâ”€â”€ document.py      # TreeNode, DocumentTree, TableBlock
â”‚   â”œâ”€â”€ query.py         # Query, Answer, QueryRecord, Citation
â”‚   â”œâ”€â”€ conversation.py  # Conversation, ConversationMessage
â”‚   â”œâ”€â”€ corpus.py        # CorpusDocument, DocumentRelationship
â”‚   â””â”€â”€ actionable.py    # Actionable, ActionablesResult
â”œâ”€â”€ tree/                 # Tree storage and management
â”‚   â”œâ”€â”€ tree_store.py    # MongoDB tree persistence
â”‚   â”œâ”€â”€ corpus_store.py  # Cross-document corpus graph
â”‚   â”œâ”€â”€ query_store.py   # Query history storage
â”‚   â”œâ”€â”€ conversation_store.py   # Chat conversation storage
â”‚   â””â”€â”€ actionable_store.py     # Actionable items storage
â”œâ”€â”€ utils/                # Utilities
â”‚   â”œâ”€â”€ llm_client.py    # OpenAI API client wrapper
â”‚   â”œâ”€â”€ mongo.py         # MongoDB connection manager
â”‚   â””â”€â”€ prompts.py       # Prompt template management
â”œâ”€â”€ config/               # Configuration
â”‚   â”œâ”€â”€ settings.py      # Pydantic settings (LLM, Tree, Retrieval)
â”‚   â””â”€â”€ prompts/         # LLM prompt templates
â”œâ”€â”€ web/                  # Next.js 16 frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/         # Next.js app router
â”‚   â”‚   â”‚   â”œâ”€â”€ history/ # Conversation history pages
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx # Main document library
â”‚   â”‚   â”‚   â””â”€â”€ layout.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ views/   # PDF viewer, Chat interface, Document library
â”‚   â”‚   â”‚   â””â”€â”€ ui/      # Shadcn UI components
â”‚   â”‚   â””â”€â”€ lib/         # API client, types, utilities
â”‚   â”œâ”€â”€ package.json     # Frontend dependencies
â”‚   â””â”€â”€ next.config.ts   # Next.js configuration
â”œâ”€â”€ data/                 # Local data storage (dev only)
â”‚   â”œâ”€â”€ trees/           # JSON tree files
â”‚   â””â”€â”€ pdfs/            # PDF uploads (dev fallback)
â”œâ”€â”€ start_backend.ps1    # PowerShell startup script
â”œâ”€â”€ DEPLOY.md           # Deployment guide
â”œâ”€â”€ requirements.txt     # Python dependencies
â””â”€â”€ README.md           # This file
```

## Models Used

### LLM Models

| Purpose | Model | Environment Variable | Fallback |
|---------|-------|---------------------|----------|
| Primary reasoning | GPT-5.2 | `LLM_MODEL=gpt-5.2` | gpt-4o |
| Deep synthesis/verification | GPT-5.2-pro | `LLM_MODEL_PRO=gpt-5.2-pro` | gpt-4o |
| Tree building | GPT-5.2 | Same as primary | gpt-4o |
| Fast classification | GPT-5.2 | Same as primary | gpt-4o |

All models are accessed via OpenAI API. Configure via `.env`:
```bash
OPENAI_API_KEY=sk-your-key-here
LLM_MODEL=gpt-5.2
LLM_MODEL_PRO=gpt-5.2-pro
```

### Document Tree Model

The core data structure replaces vector databases:

```python
@dataclass
class DocumentTree:
    doc_id: str                    # Unique identifier (sha256 hash)
    doc_name: str                  # Original filename
    doc_description: str           # LLM-generated description
    total_pages: int               # PDF page count
    structure: list[TreeNode]      # Hierarchical tree

@dataclass
class TreeNode:
    node_id: str                   # "0000", "0001", etc.
    title: str                     # Section/clause title
    node_type: NodeType          # ROOT, CHAPTER, SECTION, CLAUSE, etc.
    level: int                    # Depth in tree (0 = root)
    start_page: int               # 1-indexed page range
    end_page: int
    text: str                     # Full content
    summary: str                  # LLM-generated summary
    description: str              # LLM-generated description
    topics: list[str]             # Keyword tags for matching
    token_count: int              # Approximate token count
    children: list[TreeNode]     # Child nodes
    parent_id: str               # Parent node reference
    cross_references: list[CrossReference]
    tables: list[TableBlock]      # Embedded tables
```

### Query Types

The system classifies queries into 6 types with different retrieval strategies:

| Type | Description | Retrieval Strategy |
|------|-------------|-------------------|
| `SINGLE_HOP` | Direct lookup in one section | Locate â†’ Read |
| `MULTI_HOP` | Multiple related sections needed | Planner â†’ Multi-pass retrieve |
| `GLOBAL` | Document-wide aggregation | Read all high-level nodes |
| `DEFINITION` | Term/definition lookup | Definition injection â†’ Read |
| `CALCULATION` | Computation from values | Read tables â†’ Calculate |
| `CROSS_REF` | Following section references | Locate â†’ Follow refs â†’ Read |

## RAG Pipeline (6 Phases)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         QA ENGINE PIPELINE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Phase 1: Load Tree (~0.5s)
    â””â”€â–¶ Load DocumentTree from MongoDB
        â””â”€â–¶ Build node indexes (O(1) lookup by node_id)

Phase 2: Classify + Retrieve (~16s)
    â”œâ”€â–¶ Query Classification (LLM call)
    â”‚   â””â”€â–¶ Detect query type: SINGLE_HOP, MULTI_HOP, GLOBAL, etc.
    â”‚
    â”œâ”€â–¶ Locate (if SINGLE_HOP)
    â”‚   â”œâ”€â–¶ Send tree index (structure + summaries) to LLM
    â”‚   â””â”€â–¶ LLM returns relevant node_ids (max 15 nodes)
    â”‚
    â”œâ”€â–¶ Read
    â”‚   â”œâ”€â–¶ Fetch full text of located nodes
    â”‚   â”œâ”€â–¶ Context expansion (parent + sibling nodes)
    â”‚   â””â”€â–¶ Follow cross-references (max depth: 2)
    â”‚
    â””â”€â–¶ Return: Query + Retrieved Sections + Routing Log

Phase 3: Reflect (optional, ~10-30s)
    â”œâ”€â–¶ Assess evidence sufficiency (LLM call)
    â”œâ”€â–¶ Detect gaps in coverage
    â””â”€â–¶ Fill gaps via additional retrieval

Phase 4: Synthesize (~60-120s)
    â”œâ”€â–¶ If MULTI_HOP: Use Planner agent
    â”‚   â”œâ”€â–¶ Decompose into sub-queries
    â”‚   â”œâ”€â–¶ Execute each sub-query
    â”‚   â””â”€â–¶ Synthesize combined answer
    â”‚
    â””â”€â–¶ Standard Synthesizer
        â”œâ”€â–¶ Generate answer with citations
        â”œâ”€â–¶ Extract inferred points with confidence
        â””â”€â–¶ Link citations to source nodes

Phase 5: Verify (~20-40s)
    â”œâ”€â–¶ Check answer against source text (LLM call)
    â”œâ”€â–¶ Detect unsupported claims
    â”œâ”€â–¶ Flag confidence issues
    â””â”€â–¶ Return: verified / needs_review / failed

Phase 6: Finalize
    â”œâ”€â–¶ Attach metadata (timing, tokens, citations)
    â”œâ”€â–¶ Persist QueryRecord to MongoDB
    â””â”€â–¶ Return Answer
```

## API Endpoints

### Document Management

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/documents` | List all documents |
| `POST` | `/ingest` | Upload and process PDF |
| `GET` | `/documents/{doc_id}` | Get document tree |
| `GET` | `/documents/{doc_id}/raw` | Download raw PDF (GridFS) |
| `DELETE` | `/documents/{doc_id}` | Delete document |

### Querying

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/query` | Query a single document |
| `POST` | `/corpus/query` | Cross-document research query |
| `GET` | `/query/{record_id}` | Get past query record |
| `POST` | `/query/{record_id}/feedback` | Submit feedback |

### Actionable Extraction

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/documents/{doc_id}/actionables` | Get extracted actionables |
| `POST` | `/documents/{doc_id}/extract-actionables` | Stream extraction (SSE) |

### Conversation

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/conversations` | List conversations |
| `POST` | `/conversations` | Create conversation |
| `GET` | `/conversations/{conv_id}` | Get conversation |
| `DELETE` | `/conversations/{conv_id}` | Delete conversation |
| `GET` | `/conversations/{conv_id}/messages` | Get messages |
| `POST` | `/conversations/{conv_id}/messages` | Add messages |

### System

| Method | Endpoint | Description |
|--------|----------|-------------|
| `GET` | `/health` | Health check |
| `GET` | `/config` | System configuration |
| `GET` | `/corpus` | Get corpus graph |

## Frontend Architecture

### Tech Stack

| Component | Technology | Version |
|-----------|------------|---------|
| Framework | Next.js | 16.1.6 |
| UI | React | 19.2.3 |
| Styling | Tailwind CSS | 4.x |
| Components | Shadcn UI + Radix | Latest |
| PDF Viewer | react-pdf-viewer | 3.12.0 |
| Icons | Lucide React | 0.564.0 |
| Markdown | react-markdown | 10.1.0 |

### Key Frontend Components

| Component | Location | Purpose |
|-----------|----------|---------|
| `PdfViewer` | `web/src/components/views/pdf-viewer.tsx` | PDF rendering with citation jump |
| `ChatInterface` | `web/src/components/views/chat-interface.tsx` | Main Q&A chat with streaming |
| `DocumentLibrary` | `web/src/app/page.tsx` | Document grid and upload |
| `HistoryPage` | `web/src/app/history/page.tsx` | Conversation list |
| `ConversationDetail` | `web/src/app/history/[conv_id]/page.tsx` | Chat + PDF split view |
| `ResearchChat` | `web/src/components/views/research-chat.tsx` | Cross-document chat |

### Frontend State Management

- **No global state library** â€” uses React hooks and URL params
- **Conversation persistence** â€” all messages saved to backend
- **Streaming responses** â€” Server-Sent Events for real-time updates
- **PDF coordination** â€” Citation clicks jump to page via imperative handle

## Configuration

### Backend Settings (config/settings.py)

| Category | Setting | Default | Description |
|----------|---------|---------|-------------|
| **LLM** | `model` | `gpt-5.2` | Primary LLM |
| **LLM** | `model_pro` | `gpt-5.2-pro` | High-reasoning LLM |
| **LLM** | `temperature` | `0.1` | Creativity (0 = deterministic) |
| **LLM** | `max_tokens_default` | `8192` | Default response limit |
| **Tree** | `max_node_tokens` | `3000` | Split nodes larger than this |
| **Tree** | `toc_accuracy_threshold` | `0.6` | TOC detection confidence |
| **Retrieval** | `max_located_nodes` | `15` | Max nodes in Locate phase |
| **Retrieval** | `retrieval_token_budget` | `100000` | Token limit for context |
| **Retrieval** | `max_cross_ref_depth` | `2` | Max cross-reference hops |

### Environment Variables

```bash
# Required
OPENAI_API_KEY=sk-your-openai-key

# MongoDB Atlas (required for production)
MONGO_URI=mongodb+srv://user:pass@cluster.mongodb.net/?appName=govinda
MONGO_DB_NAME=govinda_v2

# Backend CORS (required for Vercel deployment)
ALLOWED_ORIGINS=https://your-app.vercel.app,http://localhost:3000

# Frontend (set in Vercel dashboard)
NEXT_PUBLIC_API_URL=https://your-ngrok-domain.ngrok-free.dev
```

## Setup Instructions

### Prerequisites

- Python 3.10+
- Node.js 18+
- MongoDB Atlas account (free tier works)
- OpenAI API key
- ngrok account (free static domain)

### 1. Backend Setup

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Create .env file
cat > .env << EOF
OPENAI_API_KEY=sk-your-key-here
MONGO_URI=mongodb+srv://user:pass@cluster.mongodb.net/?appName=govinda
MONGO_DB_NAME=govinda_v2
ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000
EOF

# Start backend
uvicorn app_backend.main:app --host 0.0.0.0 --port 8001
```

### 2. Frontend Setup

```bash
cd web

# Install dependencies
npm install

# Run locally (for development)
npm run dev

# Build for production
npm run build
```

### 3. ngrok Tunnel (for local backend access)

```bash
# Install ngrok
winget install ngrok  # Windows
# or
brew install ngrok    # macOS

# Authenticate (one-time)
ngrok config add-authtoken YOUR_TOKEN

# Start tunnel with static domain
ngrok http --domain=prouniformity-luther-glowingly.ngrok-free.app 8001
```

## Deployment

### Architecture

| Component | Platform | URL |
|-----------|----------|-----|
| Frontend | Vercel | `https://govinda-is-my-god.vercel.app` |
| Backend | Self-hosted + ngrok | `https://prouniformity-luther-glowingly.ngrok-free.dev` |
| Database | MongoDB Atlas | `mongodb+srv://...` |

### Daily Operation

1. **Start backend**:
   ```powershell
   cd govinda_v2_COPY_COPY
   uvicorn app_backend.main:app --host 0.0.0.0 --port 8001
   ```

2. **Start ngrok**: `ngrok http --domain=prouniformity-luther-glowingly.ngrok-free.dev 8001`

3. **Access frontend**: Vercel deployment is always live at `https://govinda-is-my-god.vercel.app`

### Vercel Environment Variables

| Variable | Value |
|----------|-------|
| `NEXT_PUBLIC_API_URL` | `https://prouniformity-luther-glowingly.ngrok-free.dev` |

## Key Features

### 1. Tree-Based RAG
- No vector database needed
- Structure-preserving hierarchical representation
- LLM-guided tree navigation (Locate â†’ Read)
- Context expansion (parent/sibling nodes)

### 2. Multi-Hop Reasoning
- Query decomposition into sub-queries
- Sequential retrieval across document sections
- Cross-document research (Corpus QA)

### 3. PDF Integration
- Side-by-side chat and PDF view
- Citation click â†’ jump to page
- Table extraction and markdown conversion
- Cross-reference following

### 4. Actionable Extraction
- Automated compliance requirement detection
- Deadline extraction
- Responsible party identification
- Streaming progress updates (SSE)

### 5. Conversation Persistence
- All queries saved as conversations
- Research chats (cross-document)
- Document-specific chats
- Export training data

## Performance Benchmarks

| Phase | Time | Tokens |
|-------|------|--------|
| Tree loading | ~0.5s | - |
| Retrieval (Phase 1) | ~16s | 5K-15K |
| Reflection (optional) | ~10-30s | 2K-5K |
| Synthesis (Phase 2) | ~60-120s | 20K-50K |
| Verification | ~20-40s | 5K-10K |
| **Total** | **~100-180s** | **30K-80K** |

## Troubleshooting

### PDF Not Loading ("Invalid PDF Structure")
- Backend fetches PDF via blob with `ngrok-skip-browser-warning` header
- Check backend is running and ngrok tunnel is active
- Verify `NEXT_PUBLIC_API_URL` has no trailing slash

### CORS Errors
- Backend must include `https://*.vercel.app` in allowed origins
- Check `ALLOWED_ORIGINS` environment variable
- Backend already configured with regex: `r"https://.*\.vercel\.app"`

### MongoDB Connection Issues
- Verify `MONGO_URI` includes proper credentials
- Check IP whitelist in Atlas (allow access from anywhere for ngrok)
- TLS is enabled automatically for Atlas connections

## License

MIT License â€” See LICENSE file for details.

## Credits

- **PDF Processing**: PyMuPDF (fitz)
- **PDF Viewer**: react-pdf-viewer by Phuoc Nguyen
- **UI Components**: Shadcn UI + Radix Primitives
- **LLM Provider**: OpenAI
- **Database**: MongoDB Atlas
